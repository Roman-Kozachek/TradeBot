{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Creating an agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will create an agent based on DQNN (Deep Q-learning neural network).  \n",
    "Let's import needed libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import random\n",
    "from collections import deque\n",
    "seed = 17\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Required ML libraries\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating a class for our agent and defining some of the local variables, which are:  \n",
    "1. `discount` - How much does the agent care about future rewards (default is set to 0.5 for safety);  \n",
    "2. `replay_memory_size` - How many last steps to keep for model training (set to default 50000); \n",
    "3. `batch_size` - How many steps (samples) to use for training (set to default 64);\n",
    "4. `min_replay_memory_size` - Minimum number of steps in a memory to start training (set to default 1000);  \n",
    "5. `update_target_every` - How often should we update our target model (set to 2 terminal episodes).  \n",
    "  \n",
    "Also we should get shapes of the environment output to build a model. We'll write a special method for that, called `_get_observation_shapes()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, observation, action_space_len=7, discount=0.5, replay_memory_size=100000, batch_size=64, alpha=0.001, model=None, update_target_every=10):\n",
    "        \n",
    "        self._replay_memory_size = replay_memory_size \n",
    "        self._batch_size = batch_size  \n",
    "        self._discount = discount  \n",
    "        self._min_replay_memory_size = 1000  \n",
    "        self._observation_shapes = self._get_observation_shapes(observation)\n",
    "        self._alpha = alpha  # learning rate\n",
    "        self._action_space_len = action_space_len\n",
    "        self._update_target_every = update_target_every\n",
    "        \n",
    "        self.model = self.create_model() if model is None else model# Main model \n",
    "        self.target_model = keras.models.clone_model(self.model) # Target network\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self._replay_memory = deque(maxlen=self._replay_memory_size)  # Memory deque\n",
    "        self._update_target_counter = 0\n",
    "        \n",
    "    def _get_observation_shapes(self, observation):\n",
    "        return ((observation[\"Hour_LSTM\"].shape[1], observation[\"Hour_LSTM\"].shape[2]),\\\n",
    "               (observation[\"M15_LSTM\"].shape[1], observation[\"M15_LSTM\"].shape[2]),\\\n",
    "               (observation[\"M5_LSTM\"].shape[1], observation[\"M5_LSTM\"].shape[2]),\\\n",
    "               (observation[\"M1_LSTM\"].shape[1], observation[\"M1_LSTM\"].shape[2]),\\\n",
    "               (observation[\"State_input\"].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a method to create a network with following structure:  \n",
    "1. Inputs are time sequences of observed shape and a state input with environmental information;  \n",
    "2. Next, we'll need `BatchNormalization` layers for TS data, because normalization was proven to improve NN performance;  \n",
    "3. After that we'll add `LSTM` layers to process TS data. We'll use `L2` regularization and Xavier initialization;  \n",
    "4. We'll `concatenate` `LSTM` outputs and send them through a series of `Dense` layers;  \n",
    "5. Finally, we'll add in environmental state info and send everything to final layer;  \n",
    "6. We'll compile model using `Adam` optimizer and `MeanSquaredError` loss for DQNN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(self):\n",
    "        # Inputs\n",
    "        h1_lstm_input = keras.Input(shape=self._observation_shapes[0], name=\"Hour_LSTM\")\n",
    "        m15_lstm_input = keras.Input(shape=self._observation_shapes[1], name=\"M15_LSTM\")\n",
    "        m5_lstm_input = keras.Input(shape=self._observation_shapes[2], name=\"M5_LSTM\")\n",
    "        m1_lstm_input = keras.Input(shape=self._observation_shapes[3], name=\"M1_LSTM\")\n",
    "        state_input = keras.Input(shape=self._observation_shapes[4], name=\"State_input\")\n",
    "\n",
    "        # Normalization\n",
    "        h1_lstm_input_norm = layers.BatchNormalization()(h1_lstm_input)\n",
    "        m15_lstm_input_norm = layers.BatchNormalization()(m15_lstm_input)\n",
    "        m5_lstm_input_norm = layers.BatchNormalization()(m5_lstm_input)\n",
    "        m1_lstm_input_norm = layers.BatchNormalization()(m1_lstm_input)\n",
    "\n",
    "        # LSTM for sequencial data with Xavier initialization and recurrent regularazer L2\n",
    "        h1_features = layers.LSTM(64, recurrent_regularizer=\"l2\", kernel_initializer=tf.keras.initializers.GlorotNormal())(h1_lstm_input_norm)\n",
    "        m15_features = layers.LSTM(64, recurrent_regularizer=\"l2\",  kernel_initializer=tf.keras.initializers.GlorotNormal())(m15_lstm_input_norm)\n",
    "        m5_features = layers.LSTM(64, recurrent_regularizer=\"l2\",  kernel_initializer=tf.keras.initializers.GlorotNormal())(m5_lstm_input_norm)\n",
    "        m1_features = layers.LSTM(64, recurrent_regularizer=\"l2\", kernel_initializer=tf.keras.initializers.GlorotNormal())(m1_lstm_input_norm)\n",
    "        \n",
    "        # Concatinating processed LSTM outputs\n",
    "        x = layers.concatenate([h1_features,m15_features, m5_features, m1_features])\n",
    "        # Feeding concatinated data to dense layers\n",
    "        dense_1 = layers.Dense(64, activation=\"relu\", kernel_regularizer=\"l2\", kernel_initializer='random_normal',\n",
    "    bias_initializer='zeros')(x)\n",
    "        dense_2 = layers.Dense(64, activation=\"relu\", kernel_regularizer=\"l2\", kernel_initializer='random_normal',\n",
    "    bias_initializer='zeros')(dense_1)\n",
    "        x_2 = layers.concatenate([dense_2, state_input])\n",
    "        # One more dense layer\n",
    "        dense_3 = layers.Dense(32, activation=\"sigmoid\", kernel_initializer='random_normal',\n",
    "    bias_initializer='zeros')(x_2)\n",
    "\n",
    "        # Output\n",
    "        y = layers.Dense(self._action_space_len, activation=\"linear\", kernel_initializer='random_normal',\n",
    "    bias_initializer='zeros')(dense_3)\n",
    "\n",
    "        model = keras.Model(inputs=[h1_lstm_input, m15_lstm_input, m5_lstm_input, m1_lstm_input, state_input],\n",
    "                           outputs=[y])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(lr=self._alpha),\n",
    "            loss=[\n",
    "                keras.losses.MeanAbsoluteError(),\n",
    "            ]\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "DQNAgent.create_model = create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna do a lot of predicting, so let's make a method that will predict values with `experimental_relax_shapes` parameter set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def predict(self, model, x, batch_size=1):\n",
    "    \"\"\"\n",
    "    A method to quickly predict the action\n",
    "    \"\"\"\n",
    "    return model(x, batch_size)\n",
    "\n",
    "DQNAgent.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write the train method for our agent!  \n",
    "  \n",
    "It's gonna go as follows:  \n",
    "1. Check if there is enough memory to train;  \n",
    "2. Create a random minibatch from memory and predict `Q-values` for current states in batch as well as `Q-values` for the next states;  \n",
    "3. Calculate new `Q-vector` based on the future `Q-values` and `discount` factor;  \n",
    "4. Train the model so that it fits with the new `Q-vector`;  \n",
    "5. After that we'll update our target model if it's time.\n",
    "\n",
    "Let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, terminal_state):\n",
    "    \"\"\"\n",
    "    Trains model to fit onto new Q values\n",
    "    \"\"\"\n",
    "    if len(self._replay_memory) < self._min_replay_memory_size:\n",
    "        return\n",
    "\n",
    "    minibatch = random.sample(self._replay_memory, self._batch_size)\n",
    "    current_states = self._create_batch(minibatch, 0)\n",
    "    current_qs_list = self.predict(self.model, current_states, batch_size=self._batch_size).numpy()\n",
    "\n",
    "    new_current_states = self._create_batch(minibatch, 3)\n",
    "    future_qs_list = self.predict(self.target_model, new_current_states, batch_size=self._batch_size).numpy()\n",
    "\n",
    "    y = []\n",
    "\n",
    "    for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "        if not done:\n",
    "            max_future_q = np.max(future_qs_list[index])\n",
    "            new_q = reward + self._discount * max_future_q\n",
    "        else:\n",
    "            new_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = new_q\n",
    "        y.append(current_qs)\n",
    "\n",
    "    self.model.fit(current_states, np.array(y), batch_size=self._batch_size, verbose=0, shuffle=False)\n",
    "\n",
    "    if terminal_state:\n",
    "        self._update_target_counter += 1\n",
    "\n",
    "    if self._update_target_every == self._update_target_counter:\n",
    "        self._update_target_counter = 0\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "\n",
    "def _create_batch(self, minibatch, i):\n",
    "    \"\"\"\n",
    "    Creates a minibatch for training (implementation is garbage, I know)\n",
    "    \"\"\"\n",
    "    h1 = []\n",
    "    m15 = []\n",
    "    m1 = []\n",
    "    m5 = []\n",
    "    st = []\n",
    "    for transition in minibatch:\n",
    "        h1.append(transition[i][\"Hour_LSTM\"].squeeze().astype(np.float32))\n",
    "        m15.append(transition[i][\"M15_LSTM\"].squeeze().astype(np.float32))\n",
    "        m5.append(transition[i][\"M5_LSTM\"].squeeze().astype(np.float32))\n",
    "        m1.append(transition[i][\"M1_LSTM\"].squeeze().astype(np.float32))\n",
    "        st.append(transition[i][\"State_input\"].squeeze().astype(np.float32))\n",
    "    return {\"Hour_LSTM\": np.asarray(h1), \"M15_LSTM\": np.asarray(m15),\\\n",
    "             \"M5_LSTM\": np.asarray(m5), \"M1_LSTM\": np.asarray(m1),\\\n",
    "             \"State_input\": np.asarray(st)}\n",
    "\n",
    "def update_replay_memory(self, transition):\n",
    "    \"\"\"\n",
    "    Update memory\n",
    "    \"\"\"\n",
    "    self._replay_memory.append(transition)\n",
    "    \n",
    "def get_qs(self, state, tg=False):\n",
    "    \"\"\"\n",
    "    Get predicted Q-values\n",
    "    \"\"\"\n",
    "    pred = self.predict(self.model if not tg else self.target_model, state)[0].numpy()\n",
    "    return pred\n",
    "\n",
    "DQNAgent.train = train\n",
    "DQNAgent._create_batch = _create_batch\n",
    "DQNAgent.update_replay_memory = update_replay_memory\n",
    "DQNAgent.get_qs = get_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our agent is ready. I'll save it into a file and we're ready for our next and final step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1562px",
    "right": "20px",
    "top": "103px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
